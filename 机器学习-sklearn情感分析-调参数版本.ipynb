{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d77c12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:22:26.329786Z",
     "start_time": "2022-07-28T02:22:25.857570Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040cd0c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:22:47.621288Z",
     "start_time": "2022-07-28T02:22:47.617186Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_file = '/Users/shiwei/Desktop/twitter_samples/positive_tweets.json'\n",
    "neg_file = '/Users/shiwei/Desktop/twitter_samples/negative_tweets.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f58825",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:24:01.214491Z",
     "start_time": "2022-07-28T02:24:00.533214Z"
    }
   },
   "outputs": [],
   "source": [
    "list1 = []\n",
    "with open(pos_file, \"r\") as f:\n",
    "    content = f.readlines() \n",
    "    for line in content:    \n",
    "        pos_data = json.loads(line)\n",
    "        list1.append(pos_data)\n",
    "df_pos = pd.DataFrame(list1)\n",
    "\n",
    "list2 = []\n",
    "with open(neg_file, \"r\") as f:\n",
    "    content = f.readlines() \n",
    "    for line in content:    \n",
    "        neg_data = json.loads(line)\n",
    "        list2.append(neg_data)\n",
    "df_neg = pd.DataFrame(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c3aca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:24:21.905602Z",
     "start_time": "2022-07-28T02:24:21.895023Z"
    }
   },
   "outputs": [],
   "source": [
    "### mapping a dictionary of apostrophe words\n",
    "\n",
    "appos = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"cant\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"im\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"gg\" : \"going\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e17be17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:24:37.334908Z",
     "start_time": "2022-07-28T02:24:37.327411Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','', text)\n",
    "    text = re.sub('@[^\\s]+','', text)\n",
    "    text = text.lower().split()\n",
    "    reformed = [appos[word] if word in appos else word for word in text]\n",
    "    reformed = \" \".join(reformed) \n",
    "    text = re.sub('&[^\\s]+;', '', reformed)\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    #text = re.sub(' [\\w] ', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52caea52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:25:02.369631Z",
     "start_time": "2022-07-28T02:25:02.200456Z"
    }
   },
   "outputs": [],
   "source": [
    "df_neg['clean_text'] = df_neg['text'].apply(lambda x: preprocess_text(x))\n",
    "df_pos['clean_text'] = df_pos['text'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69c57631",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:26:23.900736Z",
     "start_time": "2022-07-28T02:26:23.894958Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pos2 = pd.DataFrame(df_pos['clean_text'])\n",
    "df_neg2 = pd.DataFrame(df_neg['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a3d75a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:26:45.621056Z",
     "start_time": "2022-07-28T02:26:45.616542Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pos2.insert(0, 'label', 1)\n",
    "df_neg2.insert(0, 'label', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e362d619",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:27:05.029998Z",
     "start_time": "2022-07-28T02:27:05.008396Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g6/dzv0rfqn11j018hcgf4ky0080000gn/T/ipykernel_1815/3077277819.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = df_pos2.append(df_neg2, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>followfriday for being top engaged members in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>hey james how odd please call our contact cent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>we had a listen last night as you bleed is an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>congrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>yeaaaah yippppy my accnt verified rqst has suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-1</td>\n",
       "      <td>i wanna change my avi but usanele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-1</td>\n",
       "      <td>my puppy broke her foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>-1</td>\n",
       "      <td>where is all the jaebum baby pictures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>-1</td>\n",
       "      <td>but but mr ahmad maslan cooks too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-1</td>\n",
       "      <td>as a hull supporter i am expecting a misserabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                         clean_text\n",
       "0         1  followfriday for being top engaged members in ...\n",
       "1         1  hey james how odd please call our contact cent...\n",
       "2         1  we had a listen last night as you bleed is an ...\n",
       "3         1                                           congrats\n",
       "4         1  yeaaaah yippppy my accnt verified rqst has suc...\n",
       "...     ...                                                ...\n",
       "9995     -1                  i wanna change my avi but usanele\n",
       "9996     -1                            my puppy broke her foot\n",
       "9997     -1              where is all the jaebum baby pictures\n",
       "9998     -1                  but but mr ahmad maslan cooks too\n",
       "9999     -1  as a hull supporter i am expecting a misserabl...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df_pos2.append(df_neg2, ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "878d1e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T06:14:46.888883Z",
     "start_time": "2022-07-28T06:14:46.308293Z"
    }
   },
   "outputs": [],
   "source": [
    "data.to_excel('ceshi.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956c0c75",
   "metadata": {},
   "source": [
    "# 下面开始进行sklearn各种分类器的机器学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb94536e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:29:49.727480Z",
     "start_time": "2022-07-28T02:29:49.722765Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import feature engineering modules and test_train_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "#Import classification algorithm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "#Import modules to calculate accuracy and confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79603bf",
   "metadata": {},
   "source": [
    "## 下面使用TF-IDF方法将文本转换成向量，应用朴素贝叶斯分类器进行分类，给出分类效果评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e944535b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:41:23.857308Z",
     "start_time": "2022-07-28T02:41:23.523456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7343333333333333\n",
      "[[1119  348]\n",
      " [ 449 1084]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.71      0.76      0.74      1467\n",
      "           1       0.76      0.71      0.73      1533\n",
      "\n",
      "    accuracy                           0.73      3000\n",
      "   macro avg       0.74      0.73      0.73      3000\n",
      "weighted avg       0.74      0.73      0.73      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TfidfVectorizer()函数是用TF-IDF算法将文本转换成数字向量，括号里设置必要的参数\n",
    "#fit_transform()函数是正式传入文本，进行转换\n",
    "tv = TfidfVectorizer(ngram_range=(1,3), stop_words='english') \n",
    "#X = tv.fit_transform(data['clean_text'])\n",
    "X = tv.fit_transform(data['clean_text'])\n",
    "\n",
    "# train_test_split函数可以随机切分训练集和测试集\n",
    "# Xtrain：训练集数据；ytrain：训练集标签；Xtest：测试集数据；ytest：测试集标签\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, data['label'],\n",
    "                                               test_size = 0.3, shuffle=True)\n",
    "\n",
    "#开始训练\n",
    "nb = MultinomialNB(alpha=6.5, fit_prior=False)\n",
    "nb.fit(Xtrain,ytrain)\n",
    "pred = nb.predict(Xtest)\n",
    "\n",
    "#给出训练效果评分\n",
    "print(accuracy_score(ytest,pred))\n",
    "print(confusion_matrix(ytest,pred))\n",
    "print(classification_report(ytest,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c8342",
   "metadata": {},
   "source": [
    "Naive Bayes with Grid Search Hyperparameter Tuning & 10-Fold Cross Validation - achieving higher accuracy over the mdoel without hyperparameter tuning\n",
    "grid search：超参数选择，用于超参数优化，通过优化超参数之间的最优组合来改善模型性能。\n",
    "grid search使用暴风搜索法，首先为不同的超参数设定一个值列表，然后计算机会遍历每个超参数的组合进行性能评估，选出性能最佳的参数组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95ced700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:45:28.656259Z",
     "start_time": "2022-07-28T02:45:28.652161Z"
    }
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "# 设置超参数组合\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43a7d7e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:46:27.146483Z",
     "start_time": "2022-07-28T02:46:27.139020Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['clean_text'], data['label'],\n",
    "                                               test_size = 0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4ffc05c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:48:22.204467Z",
     "start_time": "2022-07-28T02:47:34.719591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.7383    0.8209    0.7774      1485\n",
      "           1     0.8028    0.7149    0.7563      1515\n",
      "\n",
      "    accuracy                         0.7673      3000\n",
      "   macro avg     0.7706    0.7679    0.7669      3000\n",
      "weighted avg     0.7709    0.7673    0.7667      3000\n",
      "\n",
      "0.7673333333333333\n",
      "[[1219  266]\n",
      " [ 432 1083]]\n"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(text_clf, tuned_parameters, cv=10)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print(classification_report(y_test, clf.predict(x_test), digits=4))\n",
    "print(accuracy_score(y_test, clf.predict(x_test)))\n",
    "print(confusion_matrix(y_test, clf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddec391",
   "metadata": {},
   "source": [
    "## 下面使用线性SVM（LinearSVC）方法进行机器学习分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a75046",
   "metadata": {},
   "source": [
    "with Grid Search Hyperparameter Tuning & 10-Fold Cross Validation\n",
    "grid search：超参数选择，用于超参数优化，通过优化超参数之间的最优组合来改善模型性能。\n",
    "grid search使用暴风搜索法，首先为不同的超参数设定一个值列表，然后计算机会遍历每个超参数的组合进行性能评估，选出性能最佳的参数组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29dd414c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:56:38.437114Z",
     "start_time": "2022-07-28T02:56:38.433116Z"
    }
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LinearSVC())])\n",
    "\n",
    "# 设置超参数组合\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 2), (1, 3), (1, 4)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__tol': [1, 1e-1, 1e-2, 1e-3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ed6d2e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T02:57:03.457983Z",
     "start_time": "2022-07-28T02:57:03.450402Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['clean_text'], data['label'],\n",
    "                                               test_size = 0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8eaf5e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T03:00:24.706456Z",
     "start_time": "2022-07-28T02:59:04.761252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.7575    0.8095    0.7826      1501\n",
      "           1     0.7951    0.7405    0.7668      1499\n",
      "\n",
      "    accuracy                         0.7750      3000\n",
      "   macro avg     0.7763    0.7750    0.7747      3000\n",
      "weighted avg     0.7763    0.7750    0.7747      3000\n",
      "\n",
      "0.775\n",
      "[[1215  286]\n",
      " [ 389 1110]]\n",
      "Best Score:  0.772\n",
      "Best Params:  {'clf__tol': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 3)}\n"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(text_clf, tuned_parameters, cv=10) # 输入超参数\n",
    "clf.fit(x_train, y_train) #开始训练\n",
    "\n",
    "print(classification_report(y_test, clf.predict(x_test), digits=4))\n",
    "print(accuracy_score(y_test, clf.predict(x_test)))\n",
    "print(confusion_matrix(y_test, clf.predict(x_test)))\n",
    "\n",
    "print(\"Best Score: \", clf.best_score_)\n",
    "print(\"Best Params: \", clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9901b",
   "metadata": {},
   "source": [
    "## 下面使用SVM（SVC）方法进行机器学习分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "974dcd0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T03:03:31.507338Z",
     "start_time": "2022-07-28T03:03:31.503416Z"
    }
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SVC())])\n",
    "\n",
    "# 设置超参数组合\n",
    "tuned_parameters = {\n",
    "    'vect__ngram_range': [(1, 2), (1, 3), (1, 4)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__tol': [1, 1e-1, 1e-2, 1e-3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83cb947f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T03:03:50.657752Z",
     "start_time": "2022-07-28T03:03:50.650186Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['clean_text'], data['label'],\n",
    "                                               test_size = 0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "840b0cbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T03:24:35.683290Z",
     "start_time": "2022-07-28T03:04:01.183168Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1     0.7447    0.8372    0.7882      1505\n",
      "           1     0.8127    0.7110    0.7585      1495\n",
      "\n",
      "    accuracy                         0.7743      3000\n",
      "   macro avg     0.7787    0.7741    0.7734      3000\n",
      "weighted avg     0.7786    0.7743    0.7734      3000\n",
      "\n",
      "0.7743333333333333\n",
      "[[1260  245]\n",
      " [ 432 1063]]\n",
      "Best Score:  0.7674285714285715\n",
      "Best Params:  {'clf__tol': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 3)}\n"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(text_clf, tuned_parameters, cv=10) # 输入超参数\n",
    "clf.fit(x_train, y_train) #开始训练\n",
    "\n",
    "print(classification_report(y_test, clf.predict(x_test), digits=4))\n",
    "print(accuracy_score(y_test, clf.predict(x_test)))\n",
    "print(confusion_matrix(y_test, clf.predict(x_test)))\n",
    "\n",
    "print(\"Best Score: \", clf.best_score_)\n",
    "print(\"Best Params: \", clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de9851b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T03:53:54.110595Z",
     "start_time": "2022-07-28T03:53:54.106651Z"
    }
   },
   "outputs": [],
   "source": [
    "# 获取最优模型\n",
    "optimal_SVM = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a5d0bee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T03:53:59.522366Z",
     "start_time": "2022-07-28T03:53:59.476396Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# 保存模型\n",
    "with open('svm-crossvalide2.pkl',\"wb\") as f:\n",
    "    pickle.dump(optimal_SVM, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "420ca8c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T03:57:19.814890Z",
     "start_time": "2022-07-28T03:57:19.776245Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取模型\n",
    "with open('svm-crossvalide2.pkl',\"rb\") as f:\n",
    "    clf_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "994f795b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T06:08:32.430084Z",
     "start_time": "2022-07-28T06:08:32.420163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用模型进行预测\n",
    "pred = clf.predict(['i hate you, you are a bad man', 'today is a nice day let us go working'])\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75623081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T04:34:53.129730Z",
     "start_time": "2022-07-28T04:34:53.121580Z"
    }
   },
   "source": [
    "这里有一个坑需要注意：\n",
    "上面一个模型的预测和下面一个魔性的预测，区别在于，上面一个可以直接输入文本进行预测，但是下面一个却需要先“X = tv.transform(text)”，转化成tf-idf向量的形式再进行预测。\n",
    "为什么会出现这样的差别呢？——原因在于训练过程步骤的不同：\n",
    "*****************************************************************************************************************\n",
    "在上面模型的训练过程中，因为多了一步超参数选择，设置了Pipeline，所以我先直接在文本上分割了训练集和测试集：\n",
    "        text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SVC())])\n",
    "        切割：\n",
    "        x_train, x_test, y_train, y_test = train_test_split(data['clean_text'], data['label'],\n",
    "                                                   test_size = 0.3, shuffle=True)\n",
    "    然后输入超参数并开始训练：\n",
    "        clf = GridSearchCV(text_clf, tuned_parameters, cv=10) # 输入超参数\n",
    "        clf.fit(x_train, y_train) #开始训练\n",
    "训练集与测试集就是直接在文本数据上切割的，这样在后面预测时候，直接调用predict函数输入文本进行预测就可以了。\n",
    "*****************************************************************************************************************\n",
    "但是下面的模型，因为没有超参数选择，没有设置Pipeline，所以直接先将文本直接转换成了tf-idf格式的向量：\n",
    "        tv = TfidfVectorizer(ngram_range=(1,3), stop_words='english') \n",
    "        X = tv.fit_transform(data['clean_text'])\n",
    "    然后对数据集进行训练集和测试集的切分，注意，这里切分的是已经变成向量的 X ！！！\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(X, data['label'],\n",
    "                                                   test_size = 0.3, shuffle=True)\n",
    "这样一来，在之后的新数据预测中，就需要先将文本转换成tf-idf格式的向量，再用predict函数进行预测。\n",
    "\n",
    "总而言之，就是需要记住Pipeline函数的功能，再预测的时候，注意是不是需要先加一步文本转换成向量。\n",
    "\n",
    "推荐还是用下面的方法吧，上面不仅需要的运行时间长，而且从结果来看也就那样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c5282",
   "metadata": {},
   "source": [
    "## 不使用超参数选择，仅仅常规自主设定参数的SVM（SVC）方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f74da88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T05:25:56.529513Z",
     "start_time": "2022-07-28T05:25:56.524173Z"
    }
   },
   "outputs": [],
   "source": [
    "#导入特征工程模块和数据集切分函数test_train_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #用tf-idf转换词向量，也可用CountVectorizer，但效果不太好\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#导入分类效果评估函数\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "456a65ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T05:26:07.133743Z",
     "start_time": "2022-07-28T05:26:06.829218Z"
    }
   },
   "outputs": [],
   "source": [
    "#TfidfVectorizer()函数是用TF-IDF算法将文本转换成数字向量，括号里设置必要的参数\n",
    "#fit_transform()函数是正式传入文本，进行转换\n",
    "tv = TfidfVectorizer(ngram_range=(1,3), stop_words='english') \n",
    "X = tv.fit_transform(data['clean_text'])\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, data['label'],\n",
    "                                               test_size = 0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "342cf27e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T05:26:15.212988Z",
     "start_time": "2022-07-28T05:26:10.098004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.721\n",
      "[[1306  195]\n",
      " [ 642  857]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.87      0.76      1501\n",
      "           1       0.81      0.57      0.67      1499\n",
      "\n",
      "    accuracy                           0.72      3000\n",
      "   macro avg       0.74      0.72      0.71      3000\n",
      "weighted avg       0.74      0.72      0.71      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(Xtrain, ytrain)\n",
    "pred = svm.predict(Xtest)\n",
    "\n",
    "print(accuracy_score(ytest, pred))\n",
    "print(confusion_matrix(ytest, pred))\n",
    "print(classification_report(ytest, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "77b5d01e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T06:07:52.484831Z",
     "start_time": "2022-07-28T06:07:52.460857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用训练好的模型进行预测新的数据集\n",
    "text = ['i hate you, you are a bad man', 'today is a nice day let us go working']\n",
    "\n",
    "#上面tv = TfidfVectorizer(ngram_range=(1,3), stop_words='english') 已经设置好了向量转换的参数\n",
    "#这里把新的文本直接转换成该文本的向量表示，输入模型，进行预测\n",
    "X = tv.transform(text)\n",
    "\n",
    "pred = svm.predict(X)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbacb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "415.852px",
    "left": "1070px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
